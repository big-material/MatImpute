{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T08:13:13.825579300Z",
     "start_time": "2023-12-25T08:13:05.291217700Z"
    }
   },
   "outputs": [],
   "source": [
    "from Impute import fill_with_meta, fill_with_et\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T08:13:13.833286900Z",
     "start_time": "2023-12-25T08:13:13.827718Z"
    }
   },
   "outputs": [],
   "source": [
    "from hyperimpute.plugins.imputers import Imputers, ImputerPlugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T08:13:13.888546900Z",
     "start_time": "2023-12-25T08:13:13.831284800Z"
    }
   },
   "outputs": [],
   "source": [
    "# stdlib\n",
    "import copy\n",
    "from time import time, strftime, localtime\n",
    "from typing import Any\n",
    "import warnings\n",
    "\n",
    "# third party\n",
    "from IPython.display import display\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pydantic import validate_arguments\n",
    "from scipy.stats import wasserstein_distance\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# hyperimpute absolute\n",
    "import hyperimpute.logger as log\n",
    "from hyperimpute.plugins.imputers import Imputers\n",
    "from hyperimpute.plugins.utils.metrics import RMSE\n",
    "from hyperimpute.plugins.utils.simulate import simulate_nan\n",
    "from hyperimpute.utils.distributions import enable_reproducible_results\n",
    "from hyperimpute.utils.metrics import generate_score, print_score\n",
    "\n",
    "enable_reproducible_results()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "@validate_arguments(config=dict(arbitrary_types_allowed=True))\n",
    "def ampute(\n",
    "        x: pd.DataFrame,\n",
    "        mechanism: str,\n",
    "        p_miss: float,\n",
    "        column_limit: int = 8,\n",
    "        sample_columns: bool = True,\n",
    ") -> tuple:\n",
    "    columns = x.columns\n",
    "    column_limit = min(len(columns), column_limit)\n",
    "\n",
    "    if sample_columns:\n",
    "        sampled_columns = columns[\n",
    "            np.random.choice(len(columns), size=column_limit, replace=False)\n",
    "        ]\n",
    "    else:\n",
    "        sampled_columns = columns[list(range(column_limit))]\n",
    "\n",
    "    x_simulated = simulate_nan(\n",
    "        x[sampled_columns].values, p_miss, mechanism, sample_columns=sample_columns, opt=\"selfmasked\"\n",
    "    )\n",
    "\n",
    "    isolated_mask = pd.DataFrame(x_simulated[\"mask\"], columns=sampled_columns)\n",
    "    isolated_x_miss = pd.DataFrame(x_simulated[\"X_incomp\"], columns=sampled_columns)\n",
    "\n",
    "    mask = pd.DataFrame(np.zeros(x.shape), columns=columns)\n",
    "    mask[sampled_columns] = pd.DataFrame(isolated_mask, columns=sampled_columns)\n",
    "\n",
    "    x_miss = pd.DataFrame(x.copy(), columns=columns)\n",
    "    x_miss[sampled_columns] = isolated_x_miss\n",
    "\n",
    "    return (\n",
    "        pd.DataFrame(x, columns=columns),\n",
    "        x_miss,\n",
    "        mask,\n",
    "    )\n",
    "\n",
    "\n",
    "@validate_arguments(config=dict(arbitrary_types_allowed=True))\n",
    "def scale_data(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    preproc = MinMaxScaler()\n",
    "    cols = X.columns\n",
    "    return pd.DataFrame(preproc.fit_transform(X), columns=cols)\n",
    "\n",
    "\n",
    "@validate_arguments(config=dict(arbitrary_types_allowed=True))\n",
    "def simulate_scenarios(\n",
    "        X: pd.DataFrame, column_limit: int = 8, sample_columns: bool = True, percentages: list = [0.1, 0.3, 0.5],\n",
    ") -> pd.DataFrame:\n",
    "    X = scale_data(X)\n",
    "\n",
    "    datasets: dict = {}\n",
    "\n",
    "    mechanisms = [\"MAR\", \"MNAR\", \"MCAR\"]\n",
    "    # percentages = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "    for ampute_mechanism in mechanisms:\n",
    "        for p_miss in percentages:\n",
    "            if ampute_mechanism not in datasets:\n",
    "                datasets[ampute_mechanism] = {}\n",
    "\n",
    "            datasets[ampute_mechanism][p_miss] = ampute(\n",
    "                X,\n",
    "                ampute_mechanism,\n",
    "                p_miss,\n",
    "                column_limit=column_limit,\n",
    "                sample_columns=sample_columns,\n",
    "            )\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def ws_score(imputed: pd.DataFrame, ground: pd.DataFrame) -> pd.DataFrame:\n",
    "    res = 0\n",
    "    for col in range(ground.shape[1]):\n",
    "        res += wasserstein_distance(\n",
    "            np.asarray(ground)[:, col], np.asarray(imputed)[:, col]\n",
    "        )\n",
    "    return res\n",
    "\n",
    "\n",
    "@validate_arguments(config=dict(arbitrary_types_allowed=True))\n",
    "def benchmark_model(\n",
    "        name: str,\n",
    "        model: Any,\n",
    "        X: pd.DataFrame,\n",
    "        X_miss: pd.DataFrame,\n",
    "        mask: pd.DataFrame,\n",
    ") -> tuple:\n",
    "    start = time()\n",
    "\n",
    "    imputed = model.fit_transform(X_miss.copy())\n",
    "\n",
    "    distribution_score = ws_score(imputed, X)\n",
    "    rmse_score = RMSE(np.asarray(imputed), np.asarray(X), np.asarray(mask))\n",
    "\n",
    "    log.info(f\"benchmark {model.name()} took {time() - start}\")\n",
    "    return rmse_score, distribution_score\n",
    "\n",
    "\n",
    "def benchmark_standard(\n",
    "        model_name: str,\n",
    "        X: pd.DataFrame,\n",
    "        X_miss: pd.DataFrame,\n",
    "        mask: pd.DataFrame,\n",
    ") -> tuple:\n",
    "    imputer = imputers.get(model_name)\n",
    "    return benchmark_model(model_name, imputer, X, X_miss, mask)\n",
    "\n",
    "\n",
    "@validate_arguments(config=dict(arbitrary_types_allowed=True))\n",
    "def evaluate_dataset(\n",
    "        name: str,\n",
    "        evaluated_model: Any,\n",
    "        X_raw: pd.DataFrame,\n",
    "        ref_methods: list = [\"mean\", \"missforest\", \"ice\", \"gain\", \"sinkhorn\", \"softimpute\"],\n",
    "        scenarios: list = [\"MAR\", \"MCAR\", \"MNAR\"],\n",
    "        miss_pct: list = [0.1, 0.3, 0.5],\n",
    "        sample_columns: bool = True,\n",
    ") -> tuple:\n",
    "    imputation_scenarios = simulate_scenarios(X_raw, sample_columns=sample_columns, percentages=miss_pct)\n",
    "\n",
    "    rmse_results: dict = {}\n",
    "    distr_results: dict = {}\n",
    "\n",
    "    for scenario in scenarios:\n",
    "\n",
    "        rmse_results[scenario] = {}\n",
    "        distr_results[scenario] = {}\n",
    "\n",
    "        for missingness in miss_pct:\n",
    "\n",
    "            log.debug(f\"  > eval {scenario} {missingness}\")\n",
    "            rmse_results[scenario][missingness] = {}\n",
    "            distr_results[scenario][missingness] = {}\n",
    "\n",
    "            try:\n",
    "                x, x_miss, mask = imputation_scenarios[scenario][missingness]\n",
    "                (our_rmse_score, our_distribution_score) = benchmark_model(\n",
    "                    name, copy.deepcopy(evaluated_model), x, x_miss, mask\n",
    "                )\n",
    "                rmse_results[scenario][missingness][\"our\"] = our_rmse_score\n",
    "                distr_results[scenario][missingness][\"our\"] = our_distribution_score\n",
    "\n",
    "                for method in ref_methods:\n",
    "                    x, x_miss, mask = imputation_scenarios[scenario][missingness]\n",
    "\n",
    "                    (\n",
    "                        mse_score,\n",
    "                        distribution_score,\n",
    "                    ) = benchmark_standard(method, x, x_miss, mask)\n",
    "                    rmse_results[scenario][missingness][method] = mse_score\n",
    "                    distr_results[scenario][missingness][method] = distribution_score\n",
    "            except BaseException as e:\n",
    "                print(f\"scenario failed {str(e)}\")\n",
    "\n",
    "                continue\n",
    "    return rmse_results, distr_results\n",
    "\n",
    "\n",
    "def compare_models(\n",
    "        name: str,\n",
    "        evaluated_model: Any,\n",
    "        X_raw: pd.DataFrame,\n",
    "        ref_methods: list = [\"mean\", \"missforest\", \"ice\", \"gain\", \"sinkhorn\", \"softimpute\"],\n",
    "        scenarios: list = [\"MNAR\"],\n",
    "        miss_pct: list = [0.1, 0.3, 0.5, 0.7],\n",
    "        n_iter: int = 2,\n",
    "        sample_columns: bool = True,\n",
    "        display_results: bool = True,\n",
    "        n_jobs: int = 1,\n",
    ") -> dict:\n",
    "    dispatcher = Parallel(n_jobs=n_jobs)\n",
    "    start = time()\n",
    "\n",
    "    def add_metrics(\n",
    "            store: dict, scenario: str, missingness: float, method: str, score: float\n",
    "    ) -> None:\n",
    "        if scenario not in store:\n",
    "            store[scenario] = {}\n",
    "        if missingness not in store[scenario]:\n",
    "            store[scenario][missingness] = {}\n",
    "        if method not in store[scenario][missingness]:\n",
    "            store[scenario][missingness][method] = []\n",
    "\n",
    "        store[scenario][missingness][method].append(score)\n",
    "\n",
    "    rmse_results_dict: dict = {}\n",
    "    distr_results_dict: dict = {}\n",
    "\n",
    "    def eval_local(it: int) -> Any:\n",
    "        enable_reproducible_results(it)\n",
    "        log.debug(f\"> evaluation trial {it}\")\n",
    "        return evaluate_dataset(\n",
    "            name=name,\n",
    "            evaluated_model=evaluated_model,\n",
    "            X_raw=X_raw,\n",
    "            ref_methods=ref_methods,\n",
    "            scenarios=scenarios,\n",
    "            miss_pct=miss_pct,\n",
    "            sample_columns=sample_columns,\n",
    "        )\n",
    "\n",
    "    repeated_evals_results = dispatcher(delayed(eval_local)(it) for it in range(n_iter))\n",
    "\n",
    "    for (\n",
    "            local_rmse_results,\n",
    "            local_distr_results,\n",
    "    ) in repeated_evals_results:\n",
    "        for scenario in local_rmse_results:\n",
    "            for missingness in local_rmse_results[scenario]:\n",
    "                for method in local_rmse_results[scenario][missingness]:\n",
    "                    add_metrics(\n",
    "                        rmse_results_dict,\n",
    "                        scenario,\n",
    "                        missingness,\n",
    "                        method,\n",
    "                        local_rmse_results[scenario][missingness][method],\n",
    "                    )\n",
    "                    add_metrics(\n",
    "                        distr_results_dict,\n",
    "                        scenario,\n",
    "                        missingness,\n",
    "                        method,\n",
    "                        local_distr_results[scenario][missingness][method],\n",
    "                    )\n",
    "\n",
    "    rmse_results = []\n",
    "    distr_results = []\n",
    "\n",
    "    rmse_str_results = []\n",
    "    distr_str_results = []\n",
    "\n",
    "    for scenario in rmse_results_dict:\n",
    "\n",
    "        for missingness in rmse_results_dict[scenario]:\n",
    "\n",
    "            local_rmse_str_results = [scenario, missingness]\n",
    "            local_distr_str_results = [scenario, missingness]\n",
    "\n",
    "            local_rmse_results = [scenario, missingness]\n",
    "            local_distr_results = [scenario, missingness]\n",
    "\n",
    "            for method in [\"our\"] + ref_methods:\n",
    "                rmse_mean, rmse_std = generate_score(\n",
    "                    rmse_results_dict[scenario][missingness][method]\n",
    "                )\n",
    "                rmse_str = print_score((rmse_mean, rmse_std))\n",
    "                distr_mean, distr_std = generate_score(\n",
    "                    distr_results_dict[scenario][missingness][method]\n",
    "                )\n",
    "                distr_str = print_score((distr_mean, distr_std))\n",
    "\n",
    "                local_rmse_str_results.append(rmse_str)\n",
    "                local_rmse_results.append((rmse_mean, rmse_std))\n",
    "\n",
    "                local_distr_str_results.append(distr_str)\n",
    "                local_distr_results.append((distr_mean, distr_std))\n",
    "\n",
    "            rmse_str_results.append(local_rmse_str_results)\n",
    "            rmse_results.append(local_rmse_results)\n",
    "            distr_str_results.append(local_distr_str_results)\n",
    "            distr_results.append(local_distr_results)\n",
    "\n",
    "    if display_results:\n",
    "        log.info(f\"benchmark took {time() - start}\")\n",
    "        headers = (\n",
    "                [\"Scenario\", \"miss_pct [0, 1]\"]\n",
    "                + [f\"Evaluated: {evaluated_model.name()}\"]\n",
    "                + ref_methods\n",
    "        )\n",
    "\n",
    "        sep = \"\\n==========================================================\\n\\n\"\n",
    "        cur_time = strftime(\"%Y%m%d_%H%M%S\", localtime())\n",
    "        data = pd.DataFrame(rmse_str_results, columns=headers)\n",
    "        data.to_csv(f\"./{name}_rmse.csv\", index=False)\n",
    "        display(data)\n",
    "\n",
    "        print(sep + \"Wasserstein score\")\n",
    "\n",
    "        data = pd.DataFrame(distr_str_results, columns=headers)\n",
    "        data.to_csv(f\"./{name}_dis.csv\", index=False)\n",
    "        display(data)\n",
    "\n",
    "    return {\n",
    "        \"headers\": headers,\n",
    "        \"rmse\": rmse_results,\n",
    "        \"wasserstein\": distr_results,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T08:13:13.897919700Z",
     "start_time": "2023-12-25T08:13:13.890917200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imputers = Imputers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T08:13:13.912380900Z",
     "start_time": "2023-12-25T08:13:13.903265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<hyperimpute.plugins.imputers.Imputers at 0x230197d1dd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EtImputer(ImputerPlugin):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._model = fill_with_et\n",
    "\n",
    "    @staticmethod\n",
    "    def name():\n",
    "        return \"et\"\n",
    "\n",
    "    @staticmethod\n",
    "    def hyperparameter_space():\n",
    "        return []\n",
    "\n",
    "    def _fit(self, *args, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def _transform(self, df):\n",
    "        # 按照缺失值的比例进行排序\n",
    "        miss_rate = df.isnull().sum() / df.shape[0]\n",
    "        cols = miss_rate.sort_values().index.tolist()\n",
    "        cols = [col for col in cols if miss_rate[col] > 0]\n",
    "        for col in cols:\n",
    "            df_col_filled = self._model(df, col)\n",
    "            df[col] = df_col_filled[col]\n",
    "        return df\n",
    "\n",
    "\n",
    "imputers.add(\"et\", EtImputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T08:13:13.955987100Z",
     "start_time": "2023-12-25T08:13:13.914258500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imputer = imputers.get(\"et\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T08:13:39.481128600Z",
     "start_time": "2023-12-25T08:13:39.466741600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('dataset/Bala_regression_dataset.csv')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# p = Path('./dataset')\n",
    "# datasets = list(p.glob('*.csv'))\n",
    "# datasets = [d for d in datasets if\n",
    "#             d.stem != '3D_printer' and d.stem != 'Concrete Compressive Strength' and d.stem != \"Bala_regression_dataset\"]\n",
    "\n",
    "datasets = [Path(\"./dataset/Bala_regression_dataset.csv\")]\n",
    "\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T09:14:26.119854200Z",
     "start_time": "2023-12-25T08:13:41.220413200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scenario</th>\n",
       "      <th>miss_pct [0, 1]</th>\n",
       "      <th>Evaluated: et</th>\n",
       "      <th>mean</th>\n",
       "      <th>hyperimpute</th>\n",
       "      <th>missforest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MCAR</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0691 +/- 0.0</td>\n",
       "      <td>0.187 +/- 0.0</td>\n",
       "      <td>0.0769 +/- 0.0</td>\n",
       "      <td>0.0769 +/- 0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Scenario  miss_pct [0, 1]   Evaluated: et           mean     hyperimpute  \\\n",
       "0     MCAR              0.1  0.0691 +/- 0.0  0.187 +/- 0.0  0.0769 +/- 0.0   \n",
       "\n",
       "       missforest  \n",
       "0  0.0769 +/- 0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================\n",
      "\n",
      "Wasserstein score\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scenario</th>\n",
       "      <th>miss_pct [0, 1]</th>\n",
       "      <th>Evaluated: et</th>\n",
       "      <th>mean</th>\n",
       "      <th>hyperimpute</th>\n",
       "      <th>missforest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MCAR</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01 +/- 0.0</td>\n",
       "      <td>0.0766 +/- 0.0</td>\n",
       "      <td>0.0229 +/- 0.0</td>\n",
       "      <td>0.023 +/- 0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Scenario  miss_pct [0, 1] Evaluated: et            mean     hyperimpute  \\\n",
       "0     MCAR              0.1  0.01 +/- 0.0  0.0766 +/- 0.0  0.0229 +/- 0.0   \n",
       "\n",
       "      missforest  \n",
       "0  0.023 +/- 0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:23<00:00, 23.49s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for d in tqdm(datasets):\n",
    "    # if Path.exists(Path(f\"./Et-knn-{d.stem}_rmse.csv\")):\n",
    "    #     print(f\"skip {d.stem}\")\n",
    "    #     continue\n",
    "    df = pd.read_csv(d)\n",
    "    df = df.select_dtypes(include=[np.number])\n",
    "    compare_models(\n",
    "        name=f\"Et-adsadasdasd-{d.stem}\",\n",
    "        evaluated_model=imputer,\n",
    "        X_raw=df,\n",
    "        ref_methods=[\"mean\", \"hyperimpute\", \"missforest\"],\n",
    "        scenarios=[\"MCAR\"],\n",
    "        miss_pct=[0.1],\n",
    "        n_iter=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
